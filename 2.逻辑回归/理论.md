# 极简逻辑回归：概念、数学与Python实现

本文将从概念、数学推导与动手实现三个方面，极简地呈现逻辑回归模型的核心知识。

- [极简逻辑回归：概念、数学与Python实现](#极简逻辑回归概念数学与python实现)
  - [1. 什么是逻辑回归？](#1-什么是逻辑回归)
  - [2. 逻辑回归的数学原理](#2-逻辑回归的数学原理)
    - [2.1 Sigmoid函数](#21-sigmoid函数)
    - [2.2 交叉熵损失函数](#22-交叉熵损失函数)
    - [2.3 梯度下降法](#23-梯度下降法)
  - [3. 从0用 Python 代码训练一个逻辑回归模型](#3-从0用-python-代码训练一个逻辑回归模型)
    - [3.1 导入必要库并生成模拟数据](#31-导入必要库并生成模拟数据)
    - [3.2 梯度下降法实现](#32-梯度下降法实现)
    - [3.3 可视化决策边界](#33-可视化决策边界)
    - [3.4 计算准确率](#34-计算准确率)

## 1. 什么是逻辑回归？

逻辑回归是一种 **分类算法**，用于预测样本属于某一类别的概率。其核心思想是用线性模型输出经过Sigmoid函数映射到(0,1)区间，表示概率。

常见应用：二分类问题，如垃圾邮件识别、肿瘤良恶性判断等。

## 2. 逻辑回归的数学原理

### 2.1 Sigmoid函数

逻辑回归的输出为：

$$
\hat{y} = \sigma(z) = \frac{1}{1 + e^{-z}}, \quad z = \mathbf{w}^\top \mathbf{x} + b
$$

其中 $\sigma(\cdot)$ 为Sigmoid函数，$\mathbf{w}$为权重，$b$为偏置。

### 2.2 交叉熵损失函数

损失函数采用 **交叉熵**：

$$
J(\mathbf{w}, b) = -\frac{1}{n} \sum_{i=1}^n \left[ y_i \log \hat{y}_i + (1-y_i)\log(1-\hat{y}_i) \right]
$$

### 2.3 梯度下降法

梯度下降法是一种迭代优化算法，旨在通过不断调整模型参数（权重 $\mathbf{w}$ 和偏置 $b$）来最小化损失函数 $J(\mathbf{w}, b)$。其核心思想是沿着损失函数梯度（导数）的反方向更新参数，因为梯度指向函数值增长最快的方向，所以其反方向是函数值下降最快的方向。

**算法步骤：**

1.  **初始化参数：** 随机初始化权重向量 $\mathbf{w}$ 和偏置 $b$。通常会将它们初始化为接近零的小值。

2.  **迭代更新：** 重复以下步骤，直到满足停止条件（例如，达到预设的迭代次数或损失函数的变化非常小）：
    * **计算梯度：** 计算损失函数 $J(\mathbf{w}, b)$ 对权重 $\mathbf{w}$ 和偏置 $b$ 的偏导数（梯度）。这些梯度表示了在当前参数值下，损失函数在每个参数方向上的变化率。
        $$
        \frac{\partial J}{\partial \mathbf{w}} = \frac{1}{n} \sum_{i=1}^n (\hat{y}_i - y_i) \mathbf{x}_i
        $$
        $$
        \frac{\partial J}{\partial b} = \frac{1}{n} \sum_{i=1}^n (\hat{y}_i - y_i)
        $$
        其中，$\hat{y}_i = \sigma(\mathbf{w}^\top \mathbf{x}_i + b)$ 是模型对第 $i$ 个样本的预测概率，$y_i$ 是第 $i$ 个样本的真实标签，$\mathbf{x}_i$ 是第 $i$ 个样本的特征向量。

    * **更新参数：** 根据计算出的梯度，按照以下规则更新权重和偏置：
        $$
        \mathbf{w} \leftarrow \mathbf{w} - \alpha \frac{\partial J}{\partial \mathbf{w}}
        $$
        $$
        b \leftarrow b - \alpha \frac{\partial J}{\partial b}
        $$
        其中，$\alpha$ 是 **学习率 (learning rate)**，它是一个正的超参数，决定了每次迭代中参数更新的步长大小。
        * **学习率过小**会导致收敛速度缓慢，需要更多的迭代次数才能找到最优解。
        * **学习率过大**可能导致在最优解附近震荡，甚至发散，无法收敛。

3.  **停止条件：** 当满足预设的停止条件时，迭代过程结束，此时的 $\mathbf{w}$ 和 $b$ 被认为是训练好的模型参数。

**直观理解：**

想象损失函数是一个多维的“山谷”，我们的目标是找到谷底（损失函数的最小值）。梯度指向山坡上升最快的方向，因此我们沿着梯度的反方向（下降最快的方向）迈出一步，每一步的步长由学习率 $\alpha$ 控制。通过多次迭代，我们逐步逼近谷底，也就是损失函数的最小值，此时模型的参数也达到了最优状态。

**总结：**

梯度下降法通过不断计算损失函数对参数的梯度，并沿着梯度的反方向以一定的步长（学习率）更新参数，最终找到使损失函数最小化的参数组合，从而训练出逻辑回归模型。

## 3. 从0用 Python 代码训练一个逻辑回归模型

[完整代码查看 LogisticRegression.ipynb](LogisticRegression.ipynb)


### 3.1 导入必要库并生成模拟数据

使用`sklearn.datasets.make_classification`生成二分类数据，并用plotly可视化。

### 3.2 梯度下降法实现

实现逻辑回归的训练过程，输出损失曲线。

### 3.3 可视化决策边界

展示模型学习到的决策边界。

### 3.4 计算准确率

用sklearn.metrics.accuracy_score评估模型性能。